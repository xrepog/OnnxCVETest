name: "model_onnx" # 模型名，也是目录名
platform: "onnxruntime_onnx" # 模型对应的平台，本次使用的是torch，不同格式的对应的平台可以在官方文档找到
max_batch_size : 0 # 一次送入模型的最大bsz，防止oom
input [
  {
    name: "X" # 输入名字，对于torch来说名字于代码的名字不需要对应，但必须是<name>__<index>的形式，注意是2个下划线，写错就报错
    data_type: TYPE_FP32 # 类型，torch.long对应的就是int64,不同语言的tensor类型与triton类型的对应关系可以在官方文档找到
    dims: [4,10]  # -1 代表是可变维度,虽然输入是二维的，但是默认第一个是bsz,所以只需要写后面的维度就行(无法理解的操作，如果是[-1,-1]调用模型就报错)
  }
]
output [
  {
    name: "Y" # 命名规范同输入
    data_type: TYPE_FP32
    dims: [4,10]
  }
]
instance_group [
  { kind: KIND_CPU count: 1 }
]
